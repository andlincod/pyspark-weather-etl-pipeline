# Pipeline ETL de Datos MeteorolÃ³gicos con PySpark

Un pipeline ETL listo para producciÃ³n construido con PySpark para procesar datos meteorolÃ³gicos, con anÃ¡lisis avanzados, verificaciones de calidad de datos y monitoreo integral.

##  QuÃ© Hace Este Proyecto

Este proyecto es un **sistema completo de procesamiento de datos meteorolÃ³gicos** que toma datos meteorolÃ³gicos en bruto y los transforma en conjuntos de datos limpios y listos para anÃ¡lisis. AquÃ­ te explicamos quÃ© hace en tÃ©rminos simples:

### **Entrada**: Datos MeteorolÃ³gicos en Bruto
- Toma mediciones meteorolÃ³gicas por hora desde archivos CSV
- Incluye temperatura, humedad, presiÃ³n, velocidad del viento y otras mÃ©tricas meteorolÃ³gicas
- Maneja datos de mÃºltiples perÃ­odos de tiempo (aÃ±os de historial meteorolÃ³gico)

### **Procesamiento**: TransformaciÃ³n Inteligente de Datos
- **Limpia los datos** eliminando duplicados, lecturas invÃ¡lidas y errores
- **Valida la calidad de los datos** para asegurar precisiÃ³n y completitud
- **Agrega datos por hora** en resÃºmenes diarios (temperatura promedio, mÃ¡xima, mÃ­nima)
- **AÃ±ade caracterÃ­sticas avanzadas** como promedios mÃ³viles, patrones estacionales y anÃ¡lisis de tendencias
- **Optimiza el rendimiento** para manejar grandes conjuntos de datos de manera eficiente

### **Salida**: Conjunto de Datos de AnÃ¡lisis Listo para Usar
- **ResÃºmenes meteorolÃ³gicos diarios** con mÃ¡s de 4,000 registros
- **CaracterÃ­sticas de series temporales** para anÃ¡lisis de tendencias y pronÃ³sticos
- **MÃ©tricas estadÃ­sticas** para cada variable meteorolÃ³gica
- **Indicadores estacionales** y patrones de fin de semana/dÃ­a laboral
- **Datos en formato Parquet** para anÃ¡lisis rÃ¡pidos y aprendizaje automÃ¡tico

### **Perfecto Para**:
-  **AnÃ¡lisis meteorolÃ³gico** e investigaciÃ³n climÃ¡tica
-  **Proyectos de ciencia de datos** y aprendizaje automÃ¡tico
-  **Inteligencia de negocios** e informes
-  **Aprender PySpark** y procesamiento de big data
-  **Pipelines ETL de producciÃ³n** para datos meteorolÃ³gicos

### **Beneficios Clave**:
-  **Maneja grandes conjuntos de datos** de manera eficiente (96K+ registros procesados)
-  **Limpia datos automÃ¡ticamente** y elimina lecturas invÃ¡lidas
-  **AÃ±ade anÃ¡lisis avanzados** automÃ¡ticamente
-  **Listo para producciÃ³n** con Docker, pruebas y monitoreo
-  **FÃ¡cil de usar** con documentaciÃ³n clara y ejemplos

##  Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fuente de     â”‚â”€â”€â”€â–¶â”‚   MÃ³dulo de     â”‚â”€â”€â”€â–¶â”‚   MÃ³dulo de     â”‚
â”‚   Datos         â”‚    â”‚   ExtracciÃ³n    â”‚    â”‚   TransformaciÃ³nâ”‚
â”‚   (Archivos CSV)â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Lake     â”‚â—€â”€â”€â”€â”‚   MÃ³dulo de     â”‚â—€â”€â”€â”€â”‚   ValidaciÃ³n de â”‚
â”‚   (Parquet)     â”‚    â”‚   Carga         â”‚    â”‚   Calidad de    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚   Datos         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   AnÃ¡lisis      â”‚
                       â”‚   (Jupyter)     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##  CaracterÃ­sticas

- **Pipeline ETL Escalable**: Construido con PySpark para procesamiento de big data
- **Verificaciones de Calidad de Datos**: ValidaciÃ³n y monitoreo integral
- **AnÃ¡lisis Avanzados**: AnÃ¡lisis de series temporales con funciones de ventana
- **Listo para ProducciÃ³n**: ContainerizaciÃ³n con Docker, CI/CD y logging
- **Pruebas Integrales**: Pruebas unitarias con cobertura del 90%+
- **OptimizaciÃ³n de Rendimiento**: Particionado, cachÃ© y optimizaciÃ³n de consultas

##  Procesamiento de Datos

El pipeline procesa datos meteorolÃ³gicos con las siguientes transformaciones:
- **Limpieza de Datos**: EliminaciÃ³n de duplicados, manejo de nulos, conversiÃ³n de tipos, filtrado de valores invÃ¡lidos
- **AgregaciÃ³n**: EstadÃ­sticas diarias de temperatura (promedio, mÃ¡ximo, mÃ­nimo, desv. estÃ¡ndar)
- **AnÃ¡lisis de Series Temporales**: Promedios mÃ³viles, patrones estacionales, caracterÃ­sticas de retraso
- **Calidad de Datos**: ValidaciÃ³n de esquema, verificaciones de rango, mÃ©tricas de completitud, validaciÃ³n de presiÃ³n
- **OptimizaciÃ³n de Rendimiento**: Particionado adecuado de ventanas, cachÃ© estratÃ©gico, optimizaciÃ³n de consultas

##  Stack TecnolÃ³gico

- **Python 3.12+**
- **PySpark 3.5.0** - Procesamiento distribuido de datos
- **Pandas** - AnÃ¡lisis y visualizaciÃ³n de datos
- **SciPy** - ComputaciÃ³n cientÃ­fica y anÃ¡lisis estadÃ­stico
- **Matplotlib & Seaborn** - VisualizaciÃ³n de datos
- **Plotly** - Visualizaciones interactivas
- **Scikit-learn** - Capacidades de aprendizaje automÃ¡tico
- **Docker** - ContainerizaciÃ³n
- **GitHub Actions** - Pipeline CI/CD
- **pytest** - Framework de pruebas
- **Black & Flake8** - Formateo de cÃ³digo y linting

##  Prerrequisitos

- Python 3.12+
- Java 11+ (requerido para PySpark)
- Docker (opcional, para despliegue containerizado)
- Git

##  Inicio RÃ¡pido

### Desarrollo Local

1. **Clonar el repositorio**
   ```bash
   git clone <repository-url>
   cd weather-etl-pipeline
   ```

2. **Crear entorno virtual**
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno**
   ```bash
   cp .env.example .env
   # Editar .env con tu configuraciÃ³n
   ```

5. **Ejecutar el pipeline ETL**
   ```bash
   python dags/etl_pipeline.py
   ```

6. **Ejecutar pruebas**
   ```bash
   pytest tests/ -v
   ```

### Despliegue con Docker

1. **Construir y ejecutar con Docker Compose**
   ```bash
   docker-compose up --build
   ```

2. **Ejecutar servicios especÃ­ficos**
   ```bash
   docker-compose up etl-pipeline
   ```

##  Estructura del Proyecto

```
weather-etl-pipeline/
â”œâ”€â”€ dags/                    # OrquestaciÃ³n ETL
â”‚   â””â”€â”€ etl_pipeline.py
â”œâ”€â”€ src/                     # CÃ³digo fuente
â”‚   â”œâ”€â”€ extract/            # ExtracciÃ³n de datos
â”‚   â”œâ”€â”€ transform/          # TransformaciÃ³n de datos
â”‚   â”œâ”€â”€ load/               # Carga de datos
â”‚   â”œâ”€â”€ utils/              # Utilidades
â”‚   â””â”€â”€ config.py           # ConfiguraciÃ³n
â”œâ”€â”€ data/                   # Almacenamiento de datos
â”‚   â”œâ”€â”€ raw/               # Datos en bruto
â”‚   â””â”€â”€ processed/         # Datos procesados
â”œâ”€â”€ notebooks/             # Jupyter notebooks
â”œâ”€â”€ tests/                 # Archivos de prueba
â”œâ”€â”€ .github/workflows/     # Pipelines CI/CD
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

##  ConfiguraciÃ³n

La aplicaciÃ³n utiliza configuraciÃ³n basada en variables de entorno. Configuraciones clave:

- `RAW_PATH`: Ruta a archivos de datos en bruto
- `PROCESSED_DIR`: Directorio de salida para datos procesados
- `LOG_LEVEL`: Nivel de logging (DEBUG, INFO, WARNING, ERROR)
- `SPARK_MASTER`: URL del master de Spark

##  CaracterÃ­sticas de Rendimiento

- **Particionado de Datos**: Estrategia de particionado optimizada para grandes conjuntos de datos
- **Operaciones de Ventana**: Particionado adecuado para funciones de ventana de series temporales
- **CachÃ©**: CachÃ© estratÃ©gico de DataFrames frecuentemente accedidos
- **Joins de Broadcast**: Operaciones de join eficientes para tablas de bÃºsqueda pequeÃ±as
- **OptimizaciÃ³n de Consultas**: Planes de explicaciÃ³n y monitoreo de rendimiento
- **Calidad de Datos**: Filtrado automÃ¡tico de datos invÃ¡lidos y validaciÃ³n

##  Pruebas

Ejecutar la suite completa de pruebas:

```bash
# Ejecutar todas las pruebas
pytest tests/ -v

# Ejecutar con cobertura
pytest tests/ --cov=src --cov-report=html

# Ejecutar archivo de prueba especÃ­fico
pytest tests/test_transform.py -v
```

##  AnÃ¡lisis

El notebook de Jupyter (`notebooks/exploratory_analysis.ipynb`) proporciona:
- ExploraciÃ³n y visualizaciÃ³n de datos
- AnÃ¡lisis estadÃ­stico e insights
- DescomposiciÃ³n de series temporales
- AnÃ¡lisis de correlaciÃ³n
- Informes listos para exportar

##  Mejoras Recientes

### v1.2.0 - Mejoras de Rendimiento y Calidad de Datos
- **Calidad de Datos Mejorada**: AÃ±adida validaciÃ³n de presiÃ³n y filtrado de datos invÃ¡lidos
- **OptimizaciÃ³n de Operaciones de Ventana**: Particionado adecuado para funciones de series temporales
- **ValidaciÃ³n de Esquema**: Corregido manejo de timestamps y validaciÃ³n de tipos
- **GestiÃ³n de Dependencias**: AÃ±adidas librerÃ­as de computaciÃ³n cientÃ­fica (SciPy, Matplotlib, etc.)
- **Monitoreo de Rendimiento**: Mejorado logging y seguimiento de rendimiento

### CaracterÃ­sticas Clave AÃ±adidas:
-  Filtrado automÃ¡tico de valores de presiÃ³n invÃ¡lidos (lecturas de 0.0)
-  Operaciones de ventana optimizadas con particionado por aÃ±o/mes
-  ValidaciÃ³n de calidad de datos mejorada con reportes detallados
-  IntegraciÃ³n de librerÃ­as de computaciÃ³n cientÃ­fica
-  Manejo de errores y logging mejorado

##  CI/CD

El proyecto incluye workflows de GitHub Actions para:
- Pruebas automatizadas en Python 3.12
- Verificaciones de calidad de cÃ³digo (Black, Flake8)
- Escaneo de seguridad
- ConstrucciÃ³n de imÃ¡genes Docker

##  Contribuir

1. Fork el repositorio
2. Crear una rama de feature (`git checkout -b feature/amazing-feature`)
3. Commit tus cambios (`git commit -m 'Add amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abrir un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT.

##  Autor

**Andres Miller**
- LinkedIn: www.linkedin.com/in/andres-miller
- Email: andlincod@outlook.com

##  Agradecimientos

- Datos meteorolÃ³gicos proporcionados por "https://www.kaggle.com/datasets/muthuj7/weather-dataset/data"
- Comunidad de PySpark por la excelente documentaciÃ³n
- Contribuidores de cÃ³digo abierto

- ##  ResoluciÃ³n de Problemas: Errores Comunes y Soluciones

Esta secciÃ³n describe problemas comunes que podrÃ­as encontrar al configurar o ejecutar este proyecto, junto con soluciones paso a paso para resolverlos.

- **Error: 'Java no encontrado' al inicializar PySpark**  
  **SoluciÃ³n**: AsegÃºrate de que Java 8+ estÃ© instalado y agregado a tu PATH. InstÃ¡lalo a travÃ©s de tu gestor de paquetes (ej., `sudo apt install default-jre` en Ubuntu) y verifica con `java -version`.

- **Error: MÃ³dulo no encontrado o errores de importaciÃ³n despuÃ©s de instalar dependencias**  
  **SoluciÃ³n**: Primero activa tu entorno virtual (ej., `source venv/bin/activate`), luego ejecuta `pip install -r requirements.txt`. Si los problemas persisten, verifica conflictos de versiones en requirements.txt.

- **Error: Permiso denegado al escribir archivos (ej., .env.example)**  
  **SoluciÃ³n**: Esto puede ocurrir debido a restricciones del sistema de archivos. Usa `sudo` para privilegios elevados o asegÃºrate de que el directorio sea escribible. Alternativamente, crea el archivo manualmente o a travÃ©s de un script, y agrÃ©galo a .gitignore si es necesario.

- **Error: Fallos de sesiÃ³n de Spark debido a configuraciÃ³n**  
  **SoluciÃ³n**: Revisa tu archivo .env para configuraciones de SPARK_MASTER. Si ejecutas localmente, configÃºralo como 'local[*]'. Verifica la instalaciÃ³n de Spark y reinicia tu entorno.

- **Error: Problemas de calidad de datos como esquemas invÃ¡lidos o valores nulos**  
  **SoluciÃ³n**: Ejecuta la validaciÃ³n de datos explÃ­citamente a travÃ©s del pipeline ETL (ej., `python dags/etl_pipeline.py`). Inspecciona los logs en el archivo LOG_FILE especificado para detalles y ajusta las configuraciones en config.py.

